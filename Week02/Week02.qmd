---
title: "Probability with Discrete Random Variables"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
---

## Obligatory Disclosure

-   Ben is an employee of Columbia University, which has received several research grants to develop Stan

-   Ben is also a manager of GG Statistics LLC, which uses Stan

-   According to Columbia University [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who has any equity stake in, a title (such as officer or director) with, or is expected to earn at least $\$5,000.00$ per year from a private company is required to disclose that

## Review of Last Week

-   For Frequentists like Fisher and Neyman, probability is needed to describe consequences of explicit randomization:

    -   Random sampling of units from a much larger population
    -   Random assignment of units to treatment groups
    -   Random measurement or other physical error

-   Supervised learning usually randomizes which observations are put in the training or testing set but mostly does not use probability to describe the implications of doing so and thus cannot characterize the induced uncertainty

-   Bayesians disagree sharply with these 2 schools of thought

## Random Variables (R.V.)

-   A function is a rule that *uniquely* maps each element of an input set to some element of an output set, e.g. $e^x$ maps real numbers $\left(\mathbb{R}\right)$ to non-negative real numbers $\left(\mathbb{R_+}\right)$
-   A random variable is a *function* from the sample space, $\Omega$, to some subset of $\mathbb{R}$ with a probability-based rule
-   If $\Omega$ is discrete with a finite number of elements, then we can simply enumerate an equivalent number of probabilities

```{r}
die_roll <- sample(1:6, size = 1, prob = rep(1 / 6, times = 6))
```

-   Do not conflate a *realization* of a R.V. with the *function* that generated it; by convention, a capital letter, $X$, indicates a R.V. and its lower-case counterpart, $x$, indicates a realization

## Bowling Basics

Each "frame" in bowling starts with $n = 10$ pins. You get up to two rolls per frame to knock down as many pins as you can.

```{=html5}
<iframe width="1908" height="879" src="https://www.youtube.com/embed/HeiNrSllyzA" title="âŒ How to Pick Up the 7 - 10 Split in Bowling ðŸŽ³" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
```
## Approaching Bowling Probabilistically

::: incremental
-   What is $\Omega$ for your first roll of a frame of bowling?
-   If $b^p = y$, then $\log_b\left(y\right) = p$. Let the probability of knocking down $x$ out of $n$ pins be given by a form of [Benford's Law](https://en.wikipedia.org/wiki/Benford%27s_law): $\Pr\left(x \mid n\right) = \log_{n + 2}\left(1 + \frac{1}{n + 1 - x}\right)$, presuming $0 \leq x \leq n$.
:::

. . .

```{r, Pr}
# probability of knocking down x out of n pins
Pr <- function(x, n = 10) ifelse(x > n, 0, log(1 + 1 / (n + 1 - x), n + 2))
Omega <- 0:10 # 0, 1, ..., 10
names(Omega) <- as.character(Omega)
source("bowling.R") # does the above, if your working directory is Week02
x_1 <- sample(Omega, size = 1, prob = Pr(Omega)) # realization of bowling
```

. . .

```{r}
round(c(Pr(Omega), total = sum(Pr(Omega))), digits = 4)
```

## Second Roll in a Frame of Bowling

-   How would you draw $x_2$, which is your second roll in the first frame of bowling?

. . .

```{r}
x_2 <- sample(Omega, size = 1, prob = Pr(Omega, n = 10 - x_1))
```

. . .

-   $\Pr\left(x \mid n = 10 - x_1\right)$ is a *conditional* probability because it depends on the realization of $x_1$ via $n = 10 - x_1$
-   If $x_1 > 0$, some elements of $\Omega$ have zero probability in the second roll, which is enforced by the `ifelse` in `Pr`
-   [Joe Blitzstein](https://youtu.be/dzFf3r1yph8): "Conditioning is the soul of statistics"

. . .

-   Pairs exercise: Simulate $R = 10$ frames of bowling (each with two rolls) and somehow store the number of pins

## Simulating $R$ Frames of Bowling

```{r}
#| message: false
#| code-line-numbers: 1-3|4-5|6-9
library(dplyr)
R <- 10^7  # practically infinite
frames <-  # tibble with the results of R frames of bowling from our model
  tibble(x_1 = sample(Omega, size = R, replace = TRUE, 
                      prob = Pr(Omega))) %>% # all R first rolls
  group_by(x_1) %>% # then all second rolls, one group at a time
  mutate(x_2 = sample(Omega, size = n(), replace = TRUE, 
                      prob = Pr(Omega, n = 10 - first(x_1)))) %>%
  ungroup
```

. . .

In the lingo of `dplyr` (and SQL), applying RNG functions after `group_by` or `filter` entail conditioning in probability

```{r}
print(frames, n = 3)
```

## Checking the Simulations

```{r}
frames %>% 
  count(x_1, name = "count") %>% 
  mutate(proportion = count / R,
         probability = Pr(x_1))
```

## Joint (here bivariate) Probabilities

-   How would you compute the probability that $x_1 = 8$ and $x_2 = 2$ in the same frame of bowling from `frames`?

. . .

```{r}
summarize(frames, prob = mean(x_1 == 8 & x_2 == 2))
```

. . .

-   How would you calculate it exactly using `Pr()`?

. . .

```{r}
Pr(x = 8, n = 10) * Pr(x = 2, n = 10 - 8)
```

## From [Aristotelian Logic](https://en.wikipedia.org/wiki/Boolean_algebra) to Probability

-   In R (and most other languages), `TRUE` maps to $1$ and `FALSE` maps to $0$ when doing arithmetic operations

```{r, AND}
c(TRUE & TRUE, TRUE & FALSE, FALSE & FALSE)
c(TRUE * TRUE, TRUE * FALSE, FALSE * FALSE)
```

. . .

::: incremental
-   Can generalize to numbers on the $[0,1]$ interval to calculate the probability that two (or more) propositions are both true. $\bigcap$ reads as "and". **General Multiplication Rule**: $\Pr\left(A\bigcap B\right)=\Pr\left(B\right)\times\Pr\left(A\mid B\right)=\Pr\left(A\right)\times\Pr\left(B\mid A\right)$
-   Iff $A$ and $B$ are independent, $\Pr\left(A \mid B\right) = \Pr\left(A\right)$ and $\Pr\left(B \mid A\right) = \Pr\left(B\right)$, so $\Pr\left(A\bigcap B\right) = \Pr\left(A\right) \times \Pr\left(B\right)$
:::

## Enumerating Bivariate Probabilities

```{r}
joint_Pr <- table(frames) / R # computed from simulation (do this!)
str(joint_Pr)
```

. . .

```{r, joint_Pr}
joint_Pr <- matrix(0, nrow = length(Omega), ncol = length(Omega),
                   dimnames = list(Omega, Omega))
for (x_1 in Omega) { # exact (but do not need to retype all this)
  Pr_x_1 <- Pr(x_1, n = 10)
  for (x_2 in 0:(10 - x_1)) {
    joint_Pr[x_1 + 1, x_2 + 1] <- Pr_x_1 * Pr(x_2, n = 10 - x_1)
  } # R indexes starting from 1 (not 0), so have to +1 the indices
}
```

. . .

```{r}
sum(joint_Pr)
```

. . .

```{r}
#| eval: false
joint_Pr # do View(joint_Pr) to see it better than on the next slide
```

##  {.smaller}

```{r}
#| echo: false
#| message: false
library(knitr)
library(kableExtra)
library(dplyr)
options("kableExtra.html.bsTable" = TRUE)
options(scipen = 5)
options(knitr.kable.NA = "")
tmp <- as.data.frame(joint_Pr)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 4), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", "black"))
kable(tmp, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Probability of Non-Exclusive Events

-   How would you compute the probability that $x_1 = 8$ or $x_2 = 2$ in the same frame of bowling using `frames`?

. . .

```{r}
summarize(frames, 
          prob  = mean(x_1 == 8 | x_2 == 2), # | reads as "or" not "given"
          wrong = mean(x_1 == 8) + mean(x_2 == 2),
          right = mean(x_1 == 8) + mean(x_2 == 2) - 
                  mean(x_1 == 8 & x_2 == 2)) # correct for double-counting
```

. . .

-   How would you calculate it exactly using `joint_Pr`?

. . .

```{r}
sum(joint_Pr["8", ]) + sum(joint_Pr[ , "2"]) - joint_Pr["8", "2"]
```

## Aristotelian Logic to Probability Again

```{r, OR}
c(TRUE | FALSE, FALSE | FALSE, TRUE | TRUE)
c(TRUE + FALSE, FALSE + FALSE, TRUE + TRUE - TRUE * TRUE)
```

::: incremental
-   Can generalize Aristotelian logic to numbers on the $[0,1]$ interval to calculate the probability that one of two propositions is true. $\bigcup$ is read as "or". **General Addition Rule**: $\Pr\left(A\bigcup B\right)=\Pr\left(A\right)+\Pr\left(B\right)-\Pr\left(A\bigcap B\right)$

-   If $\Pr\left(A\bigcap B\right) = 0$, $A$ and $B$ are said to be disjoint (or mutually exclusive)
:::

## Probability of Functions of R.V.s

::: incremental
-   What is the probability of getting a spare --- knocking down all $10$ pins over both rolls --- in a frame of bowling?

-   How would you compute it using `frames`?
:::

. . .

```{r}
summarize(frames, prob = mean(x_1 != 10 & x_1 + x_2 == 10))
```

. . .

-   How would you calculate it exactly using `joint_Pr`?

. . .

```{r}
prob <- 0 # R indexes starting from 1 (not 0), so have to +1 the indices
for (x_1 in 9:0) prob <- prob + joint_Pr[x_1 + 1, 10 - x_1 + 1]
prob
```

## Marginal Probabilities via `joint_Pr` {.scrollable .smaller}

```{r, marginal}
#| echo: false
#| message: false
tmp <- as.data.frame(cbind(joint_Pr, " " = -1, 
                           "row-sum" = rowSums(joint_Pr)))
tmp <- rbind(tmp, " " = -1, "col-sum" = colSums(tmp))
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 3), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(tmp[,i] > 
                                               1 - 1e-8 | tmp[,i] < 0, 
                                             "white", "black")))
kable(tmp, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Marginal(ized), Conditional, and Joint

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
::: incremental
-   To compose a joint (in this case, bivariate) probability, *multiply* a marginal probability by a conditional probability
-   To decompose a joint (in this case, bivariate) probability, *add* the relevant joint probabilities to obtain a marginal probability
-   To obtain a conditional probability, *divide* the joint probability by the marginalized probability of what you're conditioning on: $$\Pr\left(A\bigcap B\right)=\Pr\left(B\right)\times\Pr\left(A \mid B\right) =
    \Pr\left(A\right)\times\Pr\left(B\mid A\right)$$ $$\implies \Pr\left(A \mid B\right)= \frac{\Pr\left(A\right)\times\Pr\left(B\mid A\right)}
    {\Pr\left(B\right)} = 
    \frac{\Pr\left(A \bigcap B\right)}
    {\Pr\left(\bcancel{A} \bigcap B\right)}$$
:::

## Using Bayes' Rule for Bowling

-   How would you compute the probability that $x_1 = 8$ given that $x_2 = 2$ in the same frame of bowling using `frames`?

. . .

```{r}
filter(frames, x_2 == 2) %>% # conditions in filter() imply conditioning
summarize(prob = mean(x_1 == 8))
```

. . .

-   How would you calculate it exactly using `joint_Pr`?

. . .

```{r}
joint_Pr["8", "2"] / sum(joint_Pr[ , "2"])
```

as compared to the prior (marginal) probability of knocking down $8$ pins on the first roll, `Pr(8, n = 10)` $= `r Pr(8, n = 10)`$

##  {.smaller}

```{r}
#| echo: false
tmp <- as.data.frame(joint_Pr)
eight <- round(unlist(tmp["8", ]), digits = 4)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 4), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(i == 3, "black", "blue")))
tmp["8", ] <- cell_spec(eight, "html", bold = eight == 0, 
                        color = ifelse(eight == 0, "red", "green"))
kable(tmp, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Bayesian vs Frequentist Probability {.smaller}

-   Bayesians generalize this by taking $A$ to be "whatever you do not know" and $B$ to be "whatever you do know" to manage their beliefs using Bayes' Rule $$\Pr\left(A \mid B\right)= \frac{\Pr\left(A\right)\times\Pr\left(B\mid A\right)}
    {\Pr\left(B\right)} = 
    \frac{\Pr\left(A \bigcap B\right)}
    {\Pr\left(\bcancel{A} \bigcap B\right)}
    $$
-   Utilizing Bayes' Rule is *necessary but not sufficient* to be Bayesian
-   Frequentists accept the validity Bayes' Rule but object to using the language of probability to describe beliefs about unknown propositions and insist that probability is a property of a process that can be defined as a limit $$\Pr\left(A\right) = \lim_{R\uparrow\infty} 
    \frac{\mbox{times that } A \mbox{ occurs in } R \mbox{ independent randomizations}}{R}$$

## Probability an Odd Integer is Prime

::: incremental
-   John Cook [asks](https://www.johndcook.com/blog/2010/10/06/probability-a-number-is-prime/) an instructive question: What is the probability $x$ is prime, where $x$ is like $1 + 10^{100,000,000}$?

-   To Frequentists, $x$ is not a random variable. It is either prime or composite so it makes no sense to say "$x$ is probably $\dots$"

-   To Bayesians, no one knows for sure whether $x$ is prime or composite, but you could chose --- and then update --- a prior probability based on its number of digits, $d$ (when $d$ is large): $\Pr\left(x \mbox{ is prime} \mid d\right) = \frac{1}{d \ln 10} \approx \frac{1}{10^{10} \times 2.3}$
:::

. . .

-   What is the probability that $\beta > 0$ in a regression model?

## Scope of Bayes' Rule

Let $H$ be the hypothesis and $E$ be the evidence

![](https://3b1b-posts.us-east-1.linodeobjects.com//content/lessons/2019/bayes-theorem/bayes-geometric.png){fig-align="center"}

> What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, AI, and any situation where you want to quantify belief. (Sanderson 2019)

## Anti-Bayesian Perspectives

-   If we take statements about the probability of a hypothesis given the evidence to be the essence of science, it is weird that some scientists oppose using Bayes' Rule for science

-   Fisher argued "the theory of inverse probability is founded upon an error, and must be wholly rejected" because $H$ is not a random variable so the prior probability, $\Pr\left(H\right)$, the marginalized probability, $\Pr\left(\bcancel{H} \bigcap E\right)$, and the posterior probability $\Pr\left(H \mid E\right)$ are not well-defined quantities or else they are just subjective characteristics of the researcher

-   Supervised learning accepts that Bayes Rule is valid, but maintains that probability should not be a prerequisite
