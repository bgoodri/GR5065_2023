---
title: "Introduction to Hierarchical Models"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## Obligatory Disclosure

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
-   Ben is an employee of Columbia University, which has received several research grants to develop Stan

-   Ben is also a manager of GG Statistics LLC, which uses Stan

-   According to Columbia University [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who has any equity stake in, a title (such as officer or director) with, or is expected to earn at least $\$5,000.00$ per year from a private company is required to disclose that

## What Are Hierarchical Models

-   In Bayesian terms, a hierarchical model is nothing more than a model where the prior for some parameter depends on another

-   We have already seen several examples:

    -   Bowling: $x_2$ depends on $n = 10 - x_1$ and both depend on $\theta$
    -   HW3: Veto overrides depends on the number of regular vetoes
    -   Linear models: $\sigma \thicksim \mathcal{E}\left(r\right)$ and $\forall n: \epsilon_n \thicksim \mathcal{N}\left(0,\sigma\right)$

-   In other words, it is just another application of the probability rules: $$f\left(\boldsymbol{\theta}\right) = 
    \int f\left(\boldsymbol{\theta}, \boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K = 
    \int f\left(\boldsymbol{\theta} \mid \boldsymbol{\phi}\right)
    f\left(\boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K$$

## Splines

-   "Linear" models are linear functions of the coefficients, but you can always include nonlinear functions of predictors

    -   Interaction terms where three columns of $X$ are generated by two variables and their product
    -   Polynomials where $K$ columns of $X$ are generated by one variable raised to $K$ different powers
    -   Dummy variables that are generated by a factor with $K + 1$ levels

-   Splines are a much better way than polynomials to allow a predictor to affect $\mu_n$ in a nonlinear way

## Data on French Municipalities

```{r}
library(dplyr)
library(mgcv)
France <- readRDS(file.path("..", "Midterm", "France.rds")) %>% 
  mutate(PR = as.factor(PR))
G <- gam(turnout_2008 ~ PR + s(log_Ratio), 
         data = France, fit = FALSE)
X <- G$X
X <- X[ , -1]
dim(X)
```

## Model with a Spline

::: columns
::: {.column width="46%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \mu_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \mu_{n} & \equiv & \alpha + \Delta \times \text{PR}_n + \\ & & \sum \beta_{k} x_{nk} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \Delta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall k: \beta_{k} & \thicksim & \mathcal{N}\left(0, \sigma_\beta\right) \\ \sigma_\beta & \thicksim & \mathcal{E}\left(r_\beta\right) \end{eqnarray*}$
:::

::: {.column width="54%"}
::: fragment
Code to Draw Parameters

```{r}
R <- 100
m_0 <- 70
s_0 <- 10
m_1 <- 0.5
s_1 <- 0.25
r_eps <- 0.1
r_beta <- 2
draws <- tibble (
  alpha = rnorm(R, m_0, s_0),
  Delta = rnorm(R, m_1, s_1),
  sigma_beta = rexp(R, r_beta),
  sigma_epsilon = rexp(R, r_eps),
  beta = 
    matrix(rnorm(R * ncol(X), 
                 sd = sigma_beta),
           nrow = R)
)
```

Drawing outcomes is standard
:::
:::
:::

## Posterior Distribution

```{r, France}
#| cache: true
#| results: hide
post <- rstanarm::stan_gamm4(turnout_2008 ~ PR + s(log_Ratio),
                             data = France, chains = 1, seed = 12345,
                             prior_intercept = rstanarm::normal(m_0, s_0),
                             prior = rstanarm::normal(m_1, s_1), # on PR
                             prior_aux = rstanarm::exponential(r_eps))
```

```{r}
post
```

## Posterior Plot

```{r}
rstanarm::plot_nonlinear(post)
```

## Restricted Cubic Splines (RCS)

Often it is reasonable to assume a nonlinear function becomes linear through the most extreme data points on either side, which can be accomplished by restricting the derivatives.

```{r}
rstanarm::stan_lm(turnout_2008 ~ PR + rms::rcs(log_Ratio),
                  data = France, chains = 1, seed = 12345,
                  prior_intercept = rstanarm::normal(m_0, s_0),
                  prior = rstanarm::R2(0.25), refresh = 0)
```

## Gaussian Processes (GPs)

-   A Gaussian Process is a prior over smooth functions

-   This requires $N^3$ floating-point operations to evaluate the log-likelihood once, so it can be prohibitive for some datasets that you will encounter in the social sciences

-   The brms package includes a GP approximation that is faster

```{r}
#| eval: false
brms::get_prior(turnout_2008 ~ PR + gp(log_Ratio, k = 5, c = 5 / 4), 
                data = France)
```

## Cluster vs. Stratified Sampling

-   For cluster random sampling, you

    -   Sample $J$ large units from their population
    -   Sample $N_j$ small units from the $j$-th large unit

-   For stratified random sampling, you

    -   Divide the population of large units into $J$ mutually exclusive and exhaustive groups that are not RVs
    -   Sample $N_j$ small units from the $j$-th large unit

## Models with Group-Specific Intercepts {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group. Write a model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}}+a_j}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} +
    \epsilon_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + 
    \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   The same holds in GLMs where $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik} + a_j$ or $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}$ depending on if you are Bayesian or Frequentist

## Models with Group-Specific Slopes {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group and $\mathbf{b}_j$ is the deviation from the common coefficients. Write the model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}} + a_j + \sum_{k = 1}^K b_{jk} x_{ik}}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} + \epsilon_{ij} = \\ \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + \sum_{k = 1}^K b_{jk} x_{ik} + \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   And similarly for GLMs

## [Frequentist Estimation of MLMs](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

-   Frequentists assume that $a_j$ and $b_j$ deviate from the common parameters according to a (multivariate) normal distribution, whose (co)variances are common parameters
-   To Frequentists, $a_j$ and $b_j$ are not parameters because parameters must remain fixed in repeated sampling of observations from some population
-   Since $a_j$ and $b_j$ are not parameters, they can't be "estimated" only "predicted"
-   Since $a_j$ and $b_j$ aren't estimated, they must be integrated out of the likelihood function, leaving an integrated likelihood function of the common parameters

## Multilevel Data-Generating Processes {.smaller}

::: columns
::: {.column width="50%"}
Bayesian $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \eta_{n} & \equiv & \alpha + a_j + \left(\beta + b_j\right) x_{n} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \beta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall j: b_j & \thicksim & \mathcal{N}\left(-\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \thicksim & \mathcal{E}\left(r\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="50%"}
::: fragment
Frequentist $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(a_j + b_j x_n, \sigma_\epsilon\right) \\ \sigma_\epsilon & \text{is} & \text{given} \\ \forall n: \eta_{n} & \equiv & \alpha + \beta + x_{n} \\ \alpha & \text{is} & \text{given} \\ \beta & \text{is} & \text{given} \\ \forall j: b_j & \thicksim & \mathcal{N}\left(-\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \text{are} & \text{given} \\ \rho & \text{is} & \text{given} \end{eqnarray*}$
:::
:::
:::

## Table 2 from the lme4 [Vignette](https://www.jstatsoft.org/article/view/v067i01/0)

![](lme4Syntax.png)

## Hierarchical Models in Psychology

-   In political science and economics, the "big" units are often countries or sub-national political areas like states and the "small" units are people
-   In [psychology](https://arxiv.org/pdf/1506.04967.pdf), the "big" units are often people and the "small" units are questions or outcomes on repeated tasks
-   Hierarchical model syntax is like

```{r, eval = FALSE}
y ~ x + (x | person) + (1 | question)
```

-   Question of interest is how to predict `y` for a new "big" unit (person), as opposed to predicting how well an old "big" unit will answer a new "small" unit (question)

## Hierarchical Default Priors

```{r, message = FALSE}
library(brms)
options(mc.cores = parallel::detectCores())
# from http://www.tqmp.org/RegularArticles/vol14-2/p099/p099.pdf
dat <- readr::read_csv("https://osf.io/5cg32/download")
```

```{r}
get_prior(valence ~ arousal + (1 + arousal | PID), data = dat) %>% 
  as.list %>% `[`(1:3) %>% as_tibble  
```

## Hierarchical PPD {.smaller}

::: columns
::: {.column width="50%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \eta_{n} & \equiv & \alpha + a_j + \left(\beta + b_j\right) x_{n} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \beta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall j: b_j & \thicksim & \mathcal{N}\left(-\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \thicksim & \mathcal{E}\left(r\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="50%"}
::: fragment
Code to Draw Parameters

```{r}
R <- 1000
m_0 <- 50; s_0 <- 20
m_1 <- 0; s_1 <- 1
r_eps <- 0.1; r_sig <- 0.5
draws <- tibble (
  rho = runif(R, min = -1, max = 1),
  sigma_a = rexp(R, r_sig),
  sigma_b = rexp(R, r_sig),
  b_a = -sigma_b / sigma_a * rho,
  s = sqrt(1 - rho^2) * sigma_b,
  beta = rnorm(R, m_1, s_1),
  alpha = rnorm(R, m_0, s_0),
  sigma = rexp(R, r_sig)
)
outcomes <- group_by(dat, PID) %>% 
  with(draws, # looks inside draws for stuff
    summarize(a = rnorm(R, sd = sigma_a),
              b = rnorm(R, mean = b_a * a,
                        sd = s),
              eta = rep(alpha + a, 
                        each = nrow(dat)) +
                sapply(beta + b, FUN = `*`,
                       e2 = dat$arousal),
              epsilon = rnorm(R * nrow(dat), 
                              sd = sigma),
              y = eta + epsilon))
```
:::
:::
:::

## Hierarchical Example

```{r, psych}
#| cache: true
#| results: hide
post <-  brm(valence ~ arousal + (1 + arousal | PID), data = dat,
             prior = prior(normal(50, 20), class = "Intercept") +
               prior(normal(0, 1), class = "b") +
               prior(exponential(0.1), class = "sigma") +
               prior(exponential(0.5), class = "sd"))
```

```{r}
post
```

## PSISLOOCV Estimator of ELPD

```{r}
loo(post) # there were 46 nominal unknowns
```

## Posterior Predictive Checks

```{r}
pp_check(post, type = "ribbon_grouped", x = "arousal", group = "PID")
```

## Posterior Prediction

```{r}
PPD <- posterior_predict(post) # of previous people
nd <- dat[dat$PID == 1, ]
nd$PID <- 0L # now predict for a new person
y_0 <- posterior_predict(post, newdata = nd, allow_new_levels = TRUE)
```

-   How is that even possible? For each of the $R$ posterior draws, $\dots$

    1.  Draw $a_0$ and $b_0$ from a bivariate normal given $\sigma_a$, $\sigma_b$, $\rho$
    2.  Form $\boldsymbol{\eta}_0 \equiv \alpha + a_0 + \left(\beta + b_0\right) \mathbf{x}$
    3.  Draw each $\epsilon$ from a normal distribution with mean zero and standard deviation $\sigma$
    4.  Form the prediction vector $\mathbf{y}_0 = \boldsymbol{\eta}_0 + \boldsymbol{\epsilon}$
