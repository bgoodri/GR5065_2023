---
title: "Intermediate Hierarchical Models"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
set.seed(20230427)
```

## Obligatory Disclosure

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
-   Ben is an employee of Columbia University, which has received several research grants to develop Stan

-   Ben is also a manager of GG Statistics LLC, which uses Stan

-   According to Columbia University [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who has any equity stake in, a title (such as officer or director) with, or is expected to earn at least $\$5,000.00$ per year from a private company is required to disclose that

## Two Stage Least Squares (2SLS)

-   Instrumental variables designs are common in economics but the principles behind it are conflated with the 2SLS estimator of the instrumental variable model

    1.  Use OLS to predict the causal variable with all the other predictors, including the "instrument"

    2.  Use the fitted values from (1) in place of the causal variable when fitting the outcome with OLS, including all the other predictors except the "instrument"

-   2SLS is not even a good estimator on Frequentist grounds with finite data and how bad it is depends on the characteristics of the data-generating process

## Generative Model with an Instrument {.smaller}

::: columns
::: {.column width="46%"}
Math Notation $\begin{eqnarray*} \forall n: t_n,y_n & \thicksim & \mathcal{N}^2\left(\mu_{n,1}, \mu_{n,2}, \sigma_{1}, \sigma_{2}, \rho\right) \\ \sigma_1 & \thicksim & \mathcal{E}\left(r_1\right) \\ \sigma_2 & \thicksim & \mathcal{E}\left(r_2\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \\ \forall n: \mu_{n,1} & \equiv & \lambda + \zeta \left(z_{n} - \overline{z}\right) + \\ & & \sum_{k} \theta_k \left(x_{n,k} - \overline{x}_k\right) \\ \lambda & \thicksim & \mathcal{N}\left(M_0, S_0\right) \\ \zeta & \thicksim & \mathcal{N}\left(\uparrow,v\right) \\ \forall k: \theta_k & \thicksim & \mathcal{N}\left(M_k,S_k\right) \\ \forall n: \mu_{n,2} & \equiv & \gamma + \sum_{k} \beta_k \left(x_{n,k} - \overline{x}_k\right) \\ \gamma & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \forall k: \beta_k & \thicksim & \mathcal{N}\left(m_k,s_k\right) \\ \end{eqnarray*}$
:::

::: {.column width="54%"}
::: fragment
::: incremental
-   If $t_n$ and $y_n$ are distributed bivariate normal, then $y_n \mid t_n$ is distributed univariate normal with expectation $\mu_2 + \frac{\sigma_2}{\sigma_1}\rho \left(t_n - \mu_1\right)$ and standard deviation $\sqrt{1 - \rho^2} \sigma_2$
-   The causal effect is $\Delta = \frac{\sigma_2}{\sigma_1}\rho$ (in the model)
-   You need an informative prior on $\zeta$ that puts almost all the probability on one side of zero
-   You could also restrict the sign of $\rho$ by using a $\mathcal{U}\left(0,1\right)$ or $\mathcal{U}\left(-1,0\right)$ prior
-   Bayesians can use the same MCMC algorithm in Stan that they should use for other models
-   Once you have posterior draws of $\sigma_2$, $\sigma_1$, and $\rho$ , you can form posterior draws of $\Delta$
:::
:::
:::
:::

## Angrist and Kreuger (1991) Data

```{r}
library(dplyr)
ROOT <- "http://higheredbcs.wiley.com/legacy/college/"
PATH <- "lancaster/1405117206/datasets/AKdata.zip"
if (!file.exists("AKdata.zip")) {
  download.file(paste0(ROOT, PATH), destfile = "AKdata.zip")
  unzip("AKdata.zip")  
}
AKdata <- read.table("AKdata.txt", header = FALSE, skip = 4,
                     col.names = c("ID", "log_wage", "schooling",
                                   "birth_quarter", "age")) %>% 
  mutate(birth_quarter = as.factor(birth_quarter))
AER::ivreg(log_wage ~ age + schooling | age + birth_quarter, 
           data = AKdata) %>% summary
```

## Instrumental Variables with brms

```{r}
library(brms)
options(mc.cores = parallel::detectCores())
get_prior(brmsformula(mvbind(schooling, log_wage) ~ age + birth_quarter) + 
            set_rescor(TRUE), data = AKdata) %>% 
    as.list %>% `[`(1:5) %>% as_tibble
```

## WTF is LKJ?

-   Lewandowski, Kurowicka, & Joe (2009) derived a correlation matrix distribution that is like a symmetric beta distribution. If its shape parameter is $1$, then the PDF is constant and if its shape parameter is $> 1$, the PDF is $\bigcap$-shaped on $\left(-1,1\right)$.

-   In this case, the correlation matrix is just $2 \times 2$ with one $\rho$

-   Putting a prior on a correlation matrix and the standard deviations allows you to induce a prior on the covariances $\sigma_{ij} = \rho_{ij} \sigma_i \sigma_j$, which was a great improvement over Bayesian modeling in the 1990s

## Informative Priors

```{r}
my_prior <- 
  prior(normal(5, 1), class = "Intercept", resp = "logwage") +
  prior(constant(0), class = "b", 
        coef = "birth_quarter2", resp = "logwage") +
  prior(constant(0), class = "b", 
        coef = "birth_quarter3", resp = "logwage") +
  prior(constant(0), class = "b", 
        coef = "birth_quarter4", resp = "logwage") +
  prior(normal(0, 0.5), class = "b", coef = "age", resp = "logwage") +
  prior(normal(11, 2), class = "Intercept", resp = "schooling") +
  prior(normal(-0.1, 0.05), class = "b", 
        coef = "birth_quarter2", resp = "schooling") +
  prior(normal(-0.1, 0.05), class = "b", 
        coef = "birth_quarter3", resp = "schooling") +
  prior(normal(-0.1, 0.05), class = "b", 
        coef = "birth_quarter4", resp = "schooling") +
  prior(normal(0.5, 1), class = "b", coef = "age", resp = "schooling") +
  prior(exponential(1), class = "sigma", resp = "logwage") +
  prior(exponential(0.5), class = "sigma", resp = "schooling") +
  prior(lkj(1.5), class = "rescor")
```

## Posterior Distribution

```{r, iv}
#| cache: true
post <- brm(brmsformula(mvbind(schooling, log_wage) ~ 
                          age + birth_quarter) + # takes a long time
              set_rescor(TRUE), data = AKdata, prior = my_prior) 
```

```{r}
#| fig-show: hide
library(ggplot2)
as_tibble(post) %>% 
  mutate(Delta = sigma_logwage / sigma_schooling * 
           rescor__schooling__logwage) %>% 
  ggplot() + # plot on next slide
  geom_density(aes(x = Delta))
```

## Plot from Previous Slide

```{r}
#| echo: false
as_tibble(post) %>% 
  mutate(Delta = sigma_logwage / sigma_schooling * 
           rescor__schooling__logwage) %>% 
  ggplot() +
  geom_density(aes(x = Delta))

```

## McElreath on Hierarchical Models

-   Bayesian hierarchical models should be the default and you should need strong theoretical and empirical reasons to not utilize a hierarchical model, whose advantages include:

    1.  "Improved estimates for repeat sampling" (within units)

    2.  "Improved estimates for imbalance in sampling"

    3.  "Estimates of variation"

    4.  "Avoid averaging, retain variation"

-   MLE is a terrible estimator of hierarchical models and penalized MLE is not much better because a point estimate is a very incomplete summary of the model's implications

## Underfitting and Overfitting

1.  Full / complete pooling underfits

2.  No pooling overfits

3.  Bayesian hierarchical models do partial pooling to some degree that is estimated conditional on the data and marginalizes over the remaining uncertainty

-   In principle, you could use `loo_model_weights` to find non-negative weights (that sum to $1$) on a complete pooling model, a no pooling model, and a partial pooling model that maximize the PSISLOOCV estimator of the ELPD but almost all the weight is going to be put on the partial pooling model

## Warnings You Should Be Aware Of (1)

Unlike 1990s MCMC algorithms, Stan warns you when things do not go well, which you must heed

1.  Divergent Transitions: This means the tuned stepsize ended up too big relative to the curvature of the log-kernel
    -   Increase `adapt_delta` above its default value ($0.8$)

    -   Use more informative priors
2.  Hitting the maximum treedepth: This means the tuned stepsize ended up so small that it could not get all the way around the parameter space in one iteration
    -   Increase `max_treedepth` beyond its default value of $10$

## Warnings You Should Be Aware Of (2)

3.  Bulk / Tail Effective Sample Size too low: This means the tuned stepsize ended up so small that adjacent draws have too much dependence
    -   Increase the number of iterations or chains
4.  $\widehat{R} > 1.01$: This means the chains have not converged
    -   You could try running the chains longer, but there is probably a deeper problem
5.  Low Bayesian Fraction of Information: This means that you posterior distribution has really extreme tails
    -   You could try running the chains longer

## Final Exam

-   Will be on TUESDAY May 9th from 4:10 PM to 7:00 PM in IAB 270B, which is near my office (see the syllabus for a link to a video showing how to get there)

-   Recitation on May 2nd will be a review session

-   Final exam will be similar in format to the midterm

-   There are plenty of questions to be asked about the course material since the midterm, but all of it builds on the framework we built up prior to the midterm, so the final exam is "cumulative" in that sense

## Where Do We Go Now?

-   Future Bayesian courses at Columbia:

    -   John Paisley's Bayesian Models in Machine Learning ([E6720](http://www.columbia.edu/cu/bulletin/uwb/#/cu/bulletin/uwb/subj/EECS/E6720-20233-001)) in Fall 2023

    -   Andrew Gelman's Bayesian class, presumably in Spring 2024

-   Jonah Gabry is doing a Stan workshop at the [New York R Conference](https://rstats.ai/nyr) on July 11 -- 12

-   [StanCon](https://mc-stan.org/events/stancon2023/) in St. Louis June 20 -- 23 (must register by April 30, some scholarships are available)
